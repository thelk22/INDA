{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Text Classification\n",
    "\n",
    "This lab explores a new dataset for text classification tasks using naïve Bayes and logistic regression.\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to train and test naïve Bayes and logistic regression classifiers using scikit-learn.\n",
    "* Know how to apply evaluation metrics to the classifiers and display examples of misclassifications.\n",
    "* Be able to examine learned model parameters and explain how each classifier makes a decision.\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Load a new Twitter dataset, which is described in [this paper](https://arxiv.org/pdf/2010.12421.pdf), then extracts feature vectors from each sample.\n",
    "1. Training and evaluating naïve Bayes using Scikit-learn.\n",
    "1. Training and evaluating logistic regression using Scikit-learn.\n",
    "1. Optional extension: lemmatization and bigram features.\n",
    "1. Optional extensions: lexicon features.\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Look out for 'QUESTIONS' which you should try to answer before moving on to the next cell. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to Blackboard (anonymously) or on Teams in the QA channel (with your name), or ask a question in the Wednesday live sessions. \n",
    "\n",
    "As you work through the notebooks, please make a note of any code that is unclear to you.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data \n",
    "\n",
    "This time we are using part of the Tweet Eval dataset, which contains seven Twitter datasets for various social media classification tasks. Here, we'll focus on the sentiment analysis data. \n",
    "Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/tweet_eval):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 9.72kB [00:00, 2.07MB/s]                   \n",
      "Downloading: 30.4kB [00:00, 12.1MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tweet_eval/sentiment (download: 6.17 MiB, generated: 6.62 MiB, post-processed: Unknown size, total: 12.79 MiB) to ./data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 4.97MB [00:00, 21.4MB/s]\n",
      "Downloading: 91.2kB [00:00, 15.4MB/s]                   \n",
      "Downloading: 1.16MB [00:00, 8.45MB/s].73it/s]\n",
      "Downloading: 24.6kB [00:00, 5.69MB/s]                   \n",
      "Downloading: 219kB [00:00, 9.66MB/s]                    \n",
      "Downloading: 4.00kB [00:00, 2.57MB/s]                 \n",
      "100%|██████████| 6/6 [00:01<00:00,  5.56it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 795.00it/s]\n",
      "                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweet_eval downloaded and prepared to ./data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343. Subsequent calls will reuse this data.\n",
      "Training dataset with 45615 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 12284 instances loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the instances in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenise the text of each tweet and convert it to a bag of words, ready for input to a classifier. \n",
    "To do this, we will use the scikit-learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data into lists ready for the next steps...\n",
    "train_tweets = [sample['text'] for sample in train_dataset]\n",
    "train_labels = [sample['label'] for sample in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = [sample['text'] for sample in test_dataset]\n",
    "test_labels = [sample['label'] for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a bag of words, we can use the CountVectorizer class ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
    "This class outputs the bag of words as a feature vector, where the length of the vector is equal to the size of the vocabulary, and the values are the counts of each words in a document. \n",
    "\n",
    "Run the code below to obtain feature vectors for the training and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# CountVectorizer can do its own tokenization, but for consistency we want to\n",
    "# carry on using WordNetTokenizer. We write a small wrapper class to enable this:\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, tweets):\n",
    "        return word_tokenize(tweets)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())  # construct the vectorizer\n",
    "\n",
    "vectorizer.fit(train_tweets)  # Learn the vocabulary\n",
    "X_train = vectorizer.transform(train_tweets)  # extract training set bags of words\n",
    "X_test = vectorizer.transform(test_tweets)  # extract test set bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method sets the vectorizer up by extracting a vocabulary from some text data. \n",
    "\n",
    "QUESTION: Why do we fit the CountVectorizer on the training set?\n",
    "\n",
    "ANSWER: We fit the CountVectorizer to the training set, and not the test set, because we only want to use the training data to learn our parameters and build our model. The test data will be used for validation that the model is working to a suitable degree of accuracy. If we used all the data to train the CountVectorizer, we would have no way of validating that it is working. \n",
    "\n",
    "The vectorizer stores the vocabulary as a dictionary that maps a token to its index in the feature vector. The code below looks up the indexes of some example words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45903\n",
      "23568\n",
      "42626\n",
      "Vocabulary size = 51903\n"
     ]
    }
   ],
   "source": [
    "import reprlib\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "print(vocabulary['the'])\n",
    "print(vocabulary['horse'])\n",
    "print(vocabulary['smile'])\n",
    "\n",
    "print(f'Vocabulary size = {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes Classifier\n",
    "\n",
    "The code above has obtained the feature vectors and lists of labels. The data is now ready for use\n",
    "with scikit-learn's classifiers.\n",
    "\n",
    "Scikit-learn contains several different variants of naïve Bayes for different kinds of data. For our bag of words data, we need to use the [MultinomialNB class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB).\n",
    "\n",
    "\n",
    "TODO 2.1: Look at the documentation for MultinomalNB and write code to train a NB classifier using `X_train` and `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 51903)\n",
      "(45615,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Now we have a trained model, we would like to evaluate its performance on some test data. \n",
    "\n",
    "TODO 2.2: Refer to the documentation again and predict the labels for the test set. Use `X_test` as the inputs to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12284, 51903)\n",
      "(12284,)\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "predicted_test_labels = nb_classifier.predict(X_test)\n",
    "\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can compute standard metrics for classifier performance using [scikit-learn's metrics libary](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules). A useful function for multi-class classification (when there are more than two classes) is the [classification report function](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report).\n",
    "\n",
    "TODO 2.3: Refer again to the documentation, and compute accuracy, precision, recall and F1 scores on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5891403451644416\n",
      "Micro F1 Score: 0.5891403451644416\n",
      "Note: the following values all use macro aggregation\n",
      "Macro F1 Score: 0.5717257637198551\n",
      "Precision: 0.5856910631417499\n",
      "Recall: 0.5819831225339609\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "micro_f1 = f1_score(test_labels, predicted_test_labels, average='micro')\n",
    "macro_f1 = f1_score(test_labels, predicted_test_labels, average='macro')\n",
    "precision = precision_score(test_labels, predicted_test_labels, average='macro')\n",
    "recall = recall_score(test_labels, predicted_test_labels, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(test_labels, predicted_test_labels)}')\n",
    "print(f'Micro F1 Score: {micro_f1}')\n",
    "print('Note: the following values all use macro aggregation')\n",
    "print(f'Macro F1 Score: {macro_f1}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's examine the classifier that we learned. If you don't follow what's happening here, you may wish to refer back to the slides on naïve Bayes classifiers or to [Jurafsky and Martin's textbook](https://web.stanford.edu/~jurafsky/slp3/4.pdf). \n",
    "\n",
    "Previously, we trained a MultinomialNB classifier. The trained classifier object stores all the probabilities that it learned during training, which are needed to make predictions. The log of the likelihoods of each word given the class are represented by the attribute `feature_log_prob_`. So, if your classifier object is named `classifier`, you can access the likelihoods with `classifier.feature_log_prob_`.\n",
    "\n",
    "TODO 2.4: Print out the likelihood of the words 'happy' and 'hate' in each class. Hint: look up the index of the chosen words in `vocabulary`. The rows of `feature_log_prob` correspond to classes, and the columns to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the word 'happy' being in class 'negative' is: 0.00016529294824543762\n",
      "The probability of the word 'hate' being in class 'negative' is: 0.0005092809756751326\n",
      "The probability of the word 'happy' being in class 'neutral' is: 0.00010482340115725026\n",
      "The probability of the word 'hate' being in class 'neutral' is: 8.957636098892291e-05\n",
      "The probability of the word 'happy' being in class 'positive' is: 0.0017517544215263088\n",
      "The probability of the word 'hate' being in class 'positive' is: 5.334209566158069e-05\n"
     ]
    }
   ],
   "source": [
    "### CHANGE THE NAME OF THE CLASSIFIER VARIABLE BELOW TO USE YOUR TRAINED CLASSIFIER\n",
    "feat_likelihoods = np.exp(nb_classifier.feature_log_prob_)  # Use exponential to convert the logs back to probabilities\n",
    "###\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "happy_index = vocabulary['happy']\n",
    "hate_index = vocabulary['hate']\n",
    "\n",
    "classes = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}\n",
    "\n",
    "for clss, clssname in classes.items():\n",
    "    happy_prob = feat_likelihoods[clss, happy_index]\n",
    "    hate_prob = feat_likelihoods[clss, hate_index]\n",
    "    print(f'The probability of the word \\'happy\\' being in class \\'{clssname}\\' is: {happy_prob}')\n",
    "    print(f'The probability of the word \\'hate\\' being in class \\'{clssname}\\' is: {hate_prob}')\n",
    "\n",
    "# These probabilities are small, but correct. Remember, this is the probability of the word, given the class. \n",
    "# Therefore it will be small, because of all the words in that class it's the probability of choosing that particular one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentiment classes are negative (0), neutral (1) and positive (2). \n",
    "\n",
    "QUESTION: Which class has the strongest association with 'happy' and with 'hate'?\n",
    "ANSWER: The positive class (class 2) has the strongest association with the word 'happy'. The negative class (class 0) has the strongest association with the word 'hate'.\n",
    "\n",
    "A key part of evaluating a classifier is investigating the errors it makes to better understand its limitations. \n",
    "\n",
    "TODO 2.5: Complete the code below to print out some misclassified tweets along with their predicted and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lukekirwan/Documents/Bristol/TB2/INDA/Week5/2_classifiers.ipynb Cell 24'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukekirwan/Documents/Bristol/TB2/INDA/Week5/2_classifiers.ipynb#ch0000022?line=5'>6</a>\u001b[0m \u001b[39m### WRITE YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lukekirwan/Documents/Bristol/TB2/INDA/Week5/2_classifiers.ipynb#ch0000022?line=6'>7</a>\u001b[0m pred_err \u001b[39m=\u001b[39m predicted_test_labels[error_indexes]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lukekirwan/Documents/Bristol/TB2/INDA/Week5/2_classifiers.ipynb#ch0000022?line=7'>8</a>\u001b[0m label_err \u001b[39m=\u001b[39m test_labels[error_indexes]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukekirwan/Documents/Bristol/TB2/INDA/Week5/2_classifiers.ipynb#ch0000022?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(error_indexes)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukekirwan/Documents/Bristol/TB2/INDA/Week5/2_classifiers.ipynb#ch0000022?line=11'>12</a>\u001b[0m \u001b[39m####\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "error_indexes = predicted_test_labels != test_labels  # compare predictions to gold labels\n",
    "\n",
    "# get the text of tweets where the classifier made an error:\n",
    "tweets_err = np.array(test_tweets)[error_indexes]\n",
    "\n",
    "### WRITE YOUR CODE HERE\n",
    "pred_err = predicted_test_labels[error_indexes]\n",
    "label_err = test_labels[error_indexes]\n",
    "\n",
    "print(error_indexes)\n",
    "\n",
    "####\n",
    "\n",
    "for i in range(10):  # just print the first ten\n",
    "    print(f'Tweet: {tweets_err[i]}; true label = {label_err[i]}, prediction = {pred_err[i]}.')\n",
    "\n",
    "# Struggling with this one annoyingly! Seems simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Logistic Regression Classifier\n",
    "\n",
    "Another simple, linear classifier is logistic regression. This classifier does not rely on the conditional independence assumption, so can better model features that are highly correlated with each other. Scikit-learn provides the [logisticRegression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), which has a very similar interface to the naïve Bayes classifier.\n",
    "\n",
    "TODO 3.1: Train a logistic regression classifier, referring to the scikit-learn documentation as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "lr_classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.2: Obtain predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "predictions_logreg = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.3: Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5943503744708564\n",
      "F1 Score: 0.5685870965819503\n",
      "Precision: 0.5932357044681887\n",
      "Recall: 0.568726803590132\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "accuracy_lr = accuracy_score(test_labels, predictions_logreg)\n",
    "f1_score_lr = f1_score(test_labels, predictions_logreg, average='macro')\n",
    "precision_lr = precision_score(test_labels, predictions_logreg, average='macro')\n",
    "recall_lr = recall_score(test_labels, predictions_logreg, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy_lr}')\n",
    "print(f'F1 Score: {f1_score_lr}')\n",
    "print(f'Precision: {precision_lr}')\n",
    "print(f'Recall: {recall_lr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "QUESTION: How does the performance of logistic regression compare with naïve Bayes?\n",
    "\n",
    "ANSWER: Slightly better, but still not great. \n",
    "\n",
    "The logistic regression classifier works by learning a weight for each feature that indicates its importance in predicting a class. These weights are stored in the `coef_` attribute of the LogisticRegression object, which has rows corresponding to classes, and columns corresponding to words in the vocabulary. \n",
    "\n",
    "TODO 3.4: Print out the weights for 'happy' and 'hate' for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight of the word 'happy' in class 'negative' is: 0.00016529294824543762\n",
      "The weight of the word 'hate' in class 'negative' is: 0.0005092809756751326\n",
      "The weight of the word 'happy' in class 'neutral' is: 0.00010482340115725026\n",
      "The weight of the word 'hate' in class 'neutral' is: 8.957636098892291e-05\n",
      "The weight of the word 'happy' in class 'positive' is: 0.0017517544215263088\n",
      "The weight of the word 'hate' in class 'positive' is: 5.334209566158069e-05\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n",
    "feat_weights = lr_classifier.coef_\n",
    "\n",
    "happy_index = vocabulary['happy']\n",
    "hate_index = vocabulary['hate']\n",
    "\n",
    "classes = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}\n",
    "\n",
    "for clss, clssname in classes.items():\n",
    "    happy_prob = feat_likelihoods[clss, happy_index]\n",
    "    hate_prob = feat_likelihoods[clss, hate_index]\n",
    "    print(f'The weight of the word \\'happy\\' in class \\'{clssname}\\' is: {happy_prob}')\n",
    "    print(f'The weight of the word \\'hate\\' in class \\'{clssname}\\' is: {hate_prob}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Are the weights what you would expect to see?\n",
    "\n",
    "ANSWER: Yes, higher weights for the classes you'd expect the word to fall in. \n",
    "\n",
    "The code below prints out the words with the highest weights for each class. We use numpy's `argsort` function to get the indexes of the sorted weights. Run the code below to show the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights for class 0:\n",
      "\n",
      "sucks with weight 3.1526321346127872\n",
      "worst with weight 3.0342095726396345\n",
      "stupid with weight 2.6889778179393637\n",
      "terrible with weight 2.549632390159403\n",
      "disappointed with weight 2.461803150472878\n",
      "\n",
      "Weights for class 1:\n",
      "\n",
      "rush with weight 1.3409116343549219\n",
      "paterno with weight 1.278296534704651\n",
      "yakub with weight 1.27282481391156\n",
      "bama with weight 1.2597113294061348\n",
      "saturday\\u002c with weight 1.2514631288868407\n",
      "\n",
      "Weights for class 2:\n",
      "\n",
      "amazing with weight 3.0595739864995113\n",
      "congrats with weight 2.774124946022347\n",
      "exciting with weight 2.6281831636826163\n",
      "awesome with weight 2.553376158873567\n",
      "brilliant with weight 2.4569745211237803\n"
     ]
    }
   ],
   "source": [
    "n_feats_to_show = 5\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "for c, weights_c in enumerate(lr_classifier.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.flip(np.argsort(weights_c)[-n_feats_to_show:])\n",
    "\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.5: Use the same code as for naïve Bayes to print out examples of misclassified tweets and their labels. Hint: you should be able to compy and paste your code from above :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error_indexes = y_test_pred != test_labels  # compare predictions to gold labels\n",
    "\n",
    "# get the text of tweets where the classifier made an error:\n",
    "tweets_err = np.array(test_tweets)[error_indexes]\n",
    "\n",
    "### WRITE YOUR CODE HERE\n",
    "\n",
    "###\n",
    "\n",
    "for i in range(10):  # just print the first ten\n",
    "    print(f'Tweet: {tweets_err[i]}; true label = {gold_err[i]}, prediction = {pred_err[i]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Optional: Lemmatization and N-grams\n",
    "\n",
    "You only need to do this section if you finish the previous sections before the end of the lab.\n",
    "\n",
    "In the previous lab, we tried out lemmatization. This is useful for reducing the size of the vocabulary. Could it help us here?\n",
    "\n",
    "To apply lemmatization, we have to go back to the CountVectorizer and define a new tokenizer that will carry out the extra step of lemmatization. Run the code below to test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survive', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, tweets):\n",
    "        return [self.wnl.lemmatize(self.wnl.lemmatize(self.wnl.lemmatize(tok, pos='n'), pos='v'), pos='a') for tok in word_tokenize(tweets)]\n",
    "    \n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45312\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {len(vectorizer.vocabulary_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.1: Now, repeat your training of the logistic regression using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "lr_clf_lem = LogisticRegression()\n",
    "\n",
    "lr_clf_lem.fit(X_train, train_labels)\n",
    "\n",
    "lr_lem_pred = lr_clf_lem.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6015955714750896\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr_lem = accuracy_score(test_labels, lr_lem_pred)\n",
    "print(f'Accuracy: {accuracy_lr_lem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: Did lemmatization bring about any improvements on this dataset?\n",
    "\n",
    "ANSWER: We improved the accuracy with lemmatisation. The other metrics could be calculated and compared too. \n",
    "\n",
    "The bag of words is a very simple representation of the tweets that does not capture enough information to make accurate sentiment classifications. Another way to improve it could be to use bigrams instead of single words as our features. Bigrams are pairs of words that occur one after another in the text. Bigrams are a kind of 'n-gram', where 'n=2'. \n",
    "\n",
    "To extract bigrams, we again modify our CountVectorizer. This class has a parameter `ngram_range`, which determines the range of sizes of n-grams the vectorizer will include. If we set `ngram_range=(1,1)` we have our standard bag of words. If we set it to `ngram_range=(2,2)`, we use bigrams instead. Choosing If we set `ngram_range=(1,2)` will use both single tokens (unigrams) and bigrams.\n",
    "\n",
    "TODO 4.2: Create a new CountVectorizer that extracts bigram features instead of unigrams (single tokens) and uses the LemmaTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['`` qt', 'qt @', '@ user', 'user in', 'in the', 'the original', 'original draft', 'draft of', 'of the', 'the 7th', '7th book', 'book ,', ', remus', 'remus lupin', 'lupin survive', 'survive the', 'the battle', 'battle of', 'of hogwarts', 'hogwarts .']\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(2, 2))\n",
    "###\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.3: Now, repeat your training of the logistic regression or naïve Bayes classifier using the new features, and compare its performance with the previous classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "lr_clf_lem = LogisticRegression()\n",
    "\n",
    "lr_clf_lem.fit(X_train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5436339954412244\n"
     ]
    }
   ],
   "source": [
    "lr_lem_preds = lr_clf_lem.predict(X_test)\n",
    "\n",
    "accuracy_lr_lem = accuracy_score(test_labels, lr_lem_preds)\n",
    "print(f'Accuracy: {accuracy_lr_lem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "QUESTION: Do bigrams improve performance on this dataset?\n",
    "\n",
    "ANSWER: No. \n",
    "\n",
    "# 5. Optional: Lexicon Features\n",
    "\n",
    "You only need to do this part if you finish the other parts before the end of the lab session. \n",
    "\n",
    "The NLTK library contains sentiment lexicons, which are lists of words with negative or positive connotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/lukekirwan/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the sentiment scores for some words in the lexicon by running the code below. What do the scores mean and why do some words have no score?\n",
    "\n",
    "The scores correlate to sentiment. Some words have no score because they were not in the lexicon. We can see this is most likely because these words are not associated with a positive or negative sentiment. However it could be because these words were omitted from the lexicon on purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy: 2.7\n",
      "wonderful: 2.7\n",
      "horrible: -2.5\n",
      "boring: -1.3\n",
      "tablecloth: NOT IN LEXICON\n",
      "not: NOT IN LEXICON\n"
     ]
    }
   ],
   "source": [
    "testwords = ['happy', 'wonderful', 'horrible', 'boring', 'tablecloth', 'not']\n",
    "\n",
    "for word in testwords:\n",
    "    if word in analyser.lexicon:\n",
    "        print(f'{word}: {analyser.lexicon[word]}')\n",
    "    else:\n",
    "        print(f'{word}: NOT IN LEXICON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to use this function to compute counts of all positive and negative words. Let's start by recording whether the words in our vocabulary are positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'qt', '@', 'user', 'in', 'the', 'original', 'draft', 'of', '7th', 'book', ',', 'remus', 'lupin', 'survived', 'battle', 'hogwarts', '.', '#', 'happybirthdayremuslupin']\n"
     ]
    }
   ],
   "source": [
    "# get the Vader lexicon scores for each word in our vocabulary\n",
    "vectorizer = CountVectorizer(tokenizer=Tokenizer())\n",
    "\n",
    "vectorizer.fit(train_tweets)\n",
    "X_train = vectorizer.transform(train_tweets)\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "\n",
    "# Print out some of the features in the vocabulary:\n",
    "print(list(vectorizer.vocabulary_)[:20])\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "for i, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, i] = 1\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the counts of positive and negative words in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores for each instance in the data set. \n",
    "\n",
    "# Multiply the lexicon scores by the feature vectors, then sum over the \n",
    "# vocabulary to get the total positive and total negative counts:\n",
    "lex_pos_train = np.sum(X_train.multiply(lex_pos_scores), axis=1)\n",
    "lex_pos_test = np.sum(X_test.multiply(lex_pos_scores), axis=1)\n",
    "\n",
    "lex_neg_train = np.sum(X_train.multiply(lex_neg_scores), axis=1)\n",
    "lex_neg_test = np.sum(X_test.multiply(lex_neg_scores), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive word count: 81573.0\n",
      "Negative word count: 21649.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Positive word count: {np.sum(lex_pos_train) + np.sum(lex_pos_test)}')\n",
    "print(f'Negative word count: {np.sum(lex_neg_train) + np.sum(lex_neg_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can append the counts to the feature vector and treat them as extra features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack((X_train, lex_pos_train, lex_neg_train))\n",
    "X_test = hstack((X_test, lex_pos_test, lex_neg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 5.1: Use the new X_train and X_test feature vectors to train and evaluate your classifier. \n",
    "Does adding the lexicon features improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukekirwan/anaconda3/envs/data_analytics/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "lr_classifier.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5990719635297949\n"
     ]
    }
   ],
   "source": [
    "preds = lr_classifier.predict(X_test)\n",
    "\n",
    "accuracy_lr = accuracy_score(test_labels, preds)\n",
    "\n",
    "print(accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No real improvement on performance of logistic regression without the use of a lexicon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b46d4b78300bf5899a9ed7cd5087352ccc89aa72135844b4ccc151c4b79a4a0a"
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
